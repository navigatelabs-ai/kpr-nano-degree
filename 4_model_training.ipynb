{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdaab7f3",
   "metadata": {},
   "source": [
    "# Diamonds Dataset - Clustering with Machine Learning\n",
    "This notebook demonstrates how to train an unsupervised machine learning model to cluster diamonds based on various features. We will also discuss underfitting, overfitting, and evaluation metrics with visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606a410",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset\n",
    "We load the diamonds dataset and preprocess it using techniques applied previously, such as encoding categorical variables and scaling numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns\n",
    "diamonds_encoded = pd.get_dummies(diamonds, columns=['cut', 'color', 'clarity'], drop_first=True)\n",
    "\n",
    "# Select only the numerical columns\n",
    "numerical_cols = ['carat', 'depth', 'table', 'price', 'x', 'y', 'z']\n",
    "\n",
    "# Instantiate the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply scaling to the numerical columns\n",
    "diamonds_encoded[numerical_cols] = scaler.fit_transform(diamonds_encoded[numerical_cols])\n",
    "diamonds_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6029b",
   "metadata": {},
   "source": [
    "## Step 2: Model Training - KMeans Clustering\n",
    "We use the KMeans algorithm to cluster the diamonds. KMeans is an unsupervised learning algorithm that groups data into a predefined number of clusters based on feature similarity.\n",
    "\n",
    "### Explanation of KMeans:\n",
    "- **Initialization**: The algorithm selects 'k' random points as initial centroids.\n",
    "- **Assignment**: Each point is assigned to the nearest centroid, forming clusters.\n",
    "- **Update**: The centroids are updated by calculating the mean of the assigned points.\n",
    "- **Repeat**: The assignment and update steps repeat until convergence.\n",
    "\n",
    "### Why KMeans?\n",
    "KMeans is simple, efficient, and effective for grouping data based on similarity. It's suitable for datasets where we want to find natural groupings or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84751fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model with a predefined number of clusters (e.g., 5 clusters)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# Fit the model to the dataset\n",
    "clusters = kmeans.fit_predict(diamonds_encoded)\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "diamonds_encoded['cluster'] = clusters\n",
    "diamonds_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bec6d",
   "metadata": {},
   "source": [
    "## Step 3: Visualizing the Clusters\n",
    "We visualize the clusters using a scatter plot based on the carat and price features. The colors represent different clusters identified by the KMeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(diamonds_encoded['carat'], diamonds_encoded['price'], c=diamonds_encoded['cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.title('Clusters of Diamonds Based on Carat and Price')\n",
    "plt.xlabel('Carat')\n",
    "plt.ylabel('Price (Scaled)')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c7028",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation Metrics - Elbow Method\n",
    "To determine the optimal number of clusters, we use the Elbow Method, which plots the Within-Cluster Sum of Squares (WCSS) for different values of 'k'. The 'elbow' point indicates the best 'k' value where adding more clusters does not significantly improve the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(diamonds_encoded)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the WCSS\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68d422",
   "metadata": {},
   "source": [
    "## Step 5: Underfitting and Overfitting Explained\n",
    "Underfitting and overfitting are common issues in machine learning. We can visualize them using simple models:\n",
    "\n",
    "- **Underfitting**: When the model is too simple to capture the underlying patterns, it performs poorly on both training and testing data.\n",
    "- **Overfitting**: When the model learns the noise in the training data, it performs well on training but poorly on testing data.\n",
    "\n",
    "### Visualizing Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Underfitting with Linear Model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X)\n",
    "\n",
    "# Overfitting with a High Degree Polynomial Model\n",
    "poly = PolynomialFeatures(degree=15)\n",
    "X_poly = poly.fit_transform(X)\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly, y)\n",
    "y_pred_poly = poly_model.predict(X_poly)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred_linear, color='red', label='Linear Model (Underfitting)')\n",
    "plt.plot(X, y_pred_poly, color='green', label='Polynomial Model (Overfitting)')\n",
    "plt.title('Underfitting and Overfitting Visualization')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220fde5c",
   "metadata": {},
   "source": [
    "## Step 6: Practice Questions\n",
    "1. Why is it important to scale numerical features before training a model?\n",
    "2. What is the purpose of the Elbow Method in KMeans clustering?\n",
    "3. How does one-hot encoding differ from label encoding, and when should each be used?\n",
    "4. Explain how underfitting can be identified during model evaluation.\n",
    "5. Describe a scenario where overfitting might occur and how you could prevent it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac55eb7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrated the use of KMeans clustering to group diamonds based on different features. We visualized clusters and explored evaluation techniques like the Elbow Method to determine the optimal number of clusters. Additionally, we explained underfitting and overfitting using visual examples. These concepts and techniques are crucial for building effective machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2d498",
   "metadata": {},
   "source": [
    "### Additional Advanced Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9379d94d",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "- PCA: PCA transforms the data into a lower-dimensional space while retaining the most variance (information). In this case, we'll reduce the dataset to two components and then plot the clusters.\n",
    "- This method helps declutter the scatter plot and gives a clearer visual representation of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16913789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(diamonds_encoded.drop('cluster', axis=1))\n",
    "\n",
    "# Create a new DataFrame with PCA components\n",
    "pca_df = pd.DataFrame(data=pca_components, columns=['PCA1', 'PCA2'])\n",
    "pca_df['cluster'] = diamonds_encoded['cluster']\n",
    "\n",
    "# Plot the clusters based on PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='PCA1', y='PCA2', hue='cluster', data=pca_df, palette='viridis', alpha=0.6)\n",
    "plt.title('Clusters of Diamonds (PCA Reduced Dimensions)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1980d",
   "metadata": {},
   "source": [
    "### How It Works:\n",
    "- PCA Transformation: The PCA object reduces the dataset dimensions to 2, retaining the maximum variance.\n",
    "- DataFrame Creation: A new DataFrame is created to hold the principal components (PCA1 and PCA2) and the cluster labels.\n",
    "- Scatter Plot: The clusters are visualized in the reduced 2D space, making it less cluttered and easier to interpret.\n",
    "\n",
    "\n",
    "This visualization method helps you observe clear separation (or overlap) between clusters without the clutter of the original multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59002a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
