{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nexus Foundation Model Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus provides accesss to a variety of models using a unified LLM Gateway interface with standardized APIs.\n",
    "\n",
    "### Why is this important?\n",
    "\n",
    "- It allows you to easily switch between models without changing your code.\n",
    "\n",
    "- Getting access to models from different providers is a pain.\n",
    "\n",
    "- New versions of models are released all the time. You don't want to have to change your code every time a new model is released.\n",
    "\n",
    "- Nexus simplifies the process of getting access to models from different providers.\n",
    "\n",
    "- Nexus expose all the model using standard OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the models available today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexus offers more than 100 models from 10+ model providers. We are adding more models every week. \n",
    "To begin with, we have provided access to the following models:\n",
    "\n",
    "- Anthropic Claude Haiku (claude-haiku-v1)\n",
    "\n",
    "- Anthropic Claude Instant (claude-v1)\n",
    "\n",
    "- Meta Llama 3 8B (llama-3-8b)\n",
    "\n",
    "- Anthropic Claude Sonnet \n",
    "\n",
    "- GPT-3.5 (Coming Soon)\n",
    "- GPT-40 (Coming Soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Access Nexus Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Nexus expose access to models via OpenAI API and LangChain \n",
    "\n",
    "- Why OpenAI API ? It provides standard access to models with OpenAI Schema Specifications\n",
    "\n",
    "- Each one of you will have your own API Key\n",
    "\n",
    "- IMPORTANT: Please do not share your API Key with anyone\n",
    "\n",
    "- You are  responsible for your own API Key\n",
    "\n",
    "- Every API call is monitored and recorded for security and audit purposes\n",
    "\n",
    "- We have incorporated security and audit controls to ensure that your API Key is not compromised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the Limitations on Access models from Nexus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each model has a different set of limitations in terms of number of tokens, context length, and other factors.\n",
    "\n",
    "- Each API key is provided access with $5 worth of model access.\n",
    "\n",
    "- Each model has a different cost per token. We will be showing the data in the usage report going forward.\n",
    "\n",
    "- You are limited to 1000 tokens per minute per API key.\n",
    "\n",
    "- You are limited to 20 requests per minute per API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Nexus API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with setting up your API Key, each one of you will be provided with a unique Nexus API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nexus_api_key  = \"sk-<YOUR_NEXUS_API_KEY>\"\n",
    "\n",
    "nexus_api_url = \"https://api.nexus.navigatelabsai.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "\n",
    "%pip install langchain langchain-community openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will access all the models using OpenAI Standard APIs.\n",
    "\n",
    "### What is OpenAI python library?\n",
    "\n",
    "- The OpenAI Python library provides convenient access to the OpenAI REST API from any Python 3.8+ application. The library includes type definitions for all request params and response fields, and offers both synchronous and asynchronous clients powered by httpx.\n",
    "\n",
    "- More more details refer: https://github.com/openai/openai-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API with Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "model_ID = \"llama-3-8b\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=nexus_api_key,\n",
    "    base_url=nexus_api_url\n",
    ")\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi, Who are you?\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API without Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"claude-v1\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=nexus_api_key,\n",
    "    base_url=nexus_api_url\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"hi, Who are you?\"\n",
    "    }],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def obj_to_dict(obj):\n",
    "    if isinstance(obj, list):\n",
    "        return [obj_to_dict(item) for item in obj]\n",
    "    elif hasattr(obj, \"__dict__\"):\n",
    "        result = {}\n",
    "        for key, val in obj.__dict__.items():\n",
    "            if key.startswith('_'):\n",
    "                continue\n",
    "            result[key] = obj_to_dict(val)\n",
    "        return result\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: obj_to_dict(val) for key, val in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Assuming 'response' is your ChatCompletion object\n",
    "response_dict = obj_to_dict(response)\n",
    "\n",
    "pprint(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LangChain?\n",
    "\n",
    "- LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "- It provides a simple interface for connecting language models with other sources of data and building applications with them.\n",
    "\n",
    "- LangChain is designed to be flexible and extensible, allowing developers to easily integrate with a wide range of language models and data sources.\n",
    "\n",
    "- LangChain is an open-source project that is actively maintained and supported by a community of developers and researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ID = \"claude-haiku-v1\"\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_base=nexus_api_url, \n",
    "    model = model_ID,\n",
    "    temperature=0.1,\n",
    "    openai_api_key=nexus_api_key,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant, provide your response in bullet point format\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Hello! who are you? Whats your name? which specific one are you?\"\n",
    "    ), \n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is prompt engineerig?\n",
    "\n",
    "- Prompt engineering is a new field focused on creating and refining prompts to effectively use large language models (LLMs).\n",
    "\n",
    "- It helps in understanding what LLMs can and cannot do.\n",
    "\n",
    "- Used by researchers to enhance LLM safety and performance on tasks like question answering and arithmetic.\n",
    "\n",
    "- Developers use it to design effective prompts for LLMs and related tools.\n",
    "\n",
    "### Elements of a Prompt\n",
    "- Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "- Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "- Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "- Output Indicator - the type or format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Prompt Engineering for Large Language Models\n",
    "\n",
    "In this section, we will explore the concept of prompt engineering and its importance in optimizing the performance of large language models. We will discuss the key principles and techniques involved in crafting effective prompts that can elicit desired outputs from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant can translate english to tamil\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"my name is Vikram\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Unstructured Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You will be provided with unstructured data, and your task is to parse it into CSV format.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Time Complexity of Any code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_block = \"\"\"\n",
    "def foo(n, k):\n",
    "    accum = 0\n",
    "    for i in range(n):\n",
    "        for l in range(k):\n",
    "            accum += i\n",
    "    return accum\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You will be provided with Python code, and your task is to calculate its time complexity.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=code_block\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You will be provided with a block of text, and your task is to extract a list of keywords from it. return response in JSON format\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Black-on-black ware is a 20th- and 21st-century pottery tradition developed by the Puebloan Native American ceramic artists in Northern New Mexico. Traditional reduction-fired blackware has been made for centuries by pueblo artists. Black-on-black ware of the past century is produced with a smooth surface, with the designs applied through selective burnishing or the application of refractory slip. Another style involves carving or incising designs and selectively polishing the raised areas. For generations several families from Kha'po Owingeh and P'ohwhóge Owingeh pueblos have been making black-on-black ware with the techniques passed down from matriarch potters. Artists from other pueblos have also produced black-on-black ware. Several contemporary artists have created works honoring the pottery of their ancestors.\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Questions Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an experienced Data Scientist Interviewer. I want you to screen a candidate with a rapid fire round where expected answer is around 1-2 sentences. Ask questions in list format.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Provide 5 internview questions for a Data Scientist role.\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Domain Specific Prompt Engineering Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto News and Headlines Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an expert crypto analyst, you are presented with a news article about a crypto asset. Perform the tasks thats asked\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"\"\"Bitcoin spiked above $93,000 for a short period as expectations of further interest-rate reductions by the Federal Reserve added to the impetus from President-elect Donald Trump’s pro-crypto stance.\n",
    "                The digital asset rose nearly 6% in the US to a record $93,462 but failed to hold the climb, falling back to $89,370 as of 6:10 a.m. on Thursday in Singapore. \n",
    "                The wider crypto market swung between gains and losses amid choppy trading.\n",
    "                \n",
    "                From the content above\n",
    "                1. Generate a headline for the article\n",
    "                2. Generate a summary of the article in 2 lines\n",
    "                3. Extract all the names of cryptocurrencies mentioned in the article\n",
    "                4. Extract all the names of exchanges mentioned in the article\n",
    "                5. Extract all the names of companies mentioned in the article\n",
    "                6. Extract all of the above in JSON format\n",
    "                \"\"\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agriculture Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an expert Agriculture researcher, you are tasked to extract information from the text and answer the question.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"\"\"The treatments each year 2017 and 2018 consisted of: rabbit manure, cow dung, poultry manure, green manure [Mexican sunflower (Tithonia diversifolia Asteraceae)], pig manure, NPK 15-15-15 fertilizer applied at 120 kg N ha−1 and a control (no manure/inorganic fertilizer). \n",
    "        The seven treatments were laid out in a randomized complete block design with three replication. \n",
    "        Organic manures and NPK fertilizer increased the soil organic matter (OM), N, P, K, Ca and Mg (NPK fertilizer did not increase OM, Ca and Mg significantly), growth, yield, minerals, protein, ash, carbohydrate and mucilage contents of okra fruit as compared with control. \n",
    "        Organic manures improved okra yield compared with NPK fertilizer. \n",
    "                \n",
    "                From the content above\n",
    "                1. Did organic manure increase the soil organic matter  significantly?\n",
    "                2. What are rhe minerals increased by the organic manure?\n",
    "                3. Which crop was mentioned in the content?\n",
    "                \"\"\"\n",
    "    ),\n",
    "]\n",
    "response = chat(messages)\n",
    "\n",
    "# Pretty print just the content\n",
    "print(\"Content:\")\n",
    "print(response.content)\n",
    "\n",
    "# Pretty print the full response with metadata\n",
    "print(\"\\nFull Response:\")\n",
    "pprint(vars(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Prompt Engineering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting: \n",
    "Providing examples within the prompt to guide the model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Classify the sentiment of the third movie review. Use the information from the first two examples:\n",
    "            Review: \"This movie was a waste of time.\"\n",
    "            Sentiment: Negative\n",
    "            Review: \"I couldn't stop laughing throughout the film!\"\n",
    "            Sentiment: Positive\n",
    "            Review: \"The special effects were amazing, but the plot was confusing.\"\n",
    "            Sentiment:\n",
    "           \"\"\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct prompting\n",
    "\n",
    "ReAct Prompting is a novel technique within the realm of language models, specifically tailored to instruct LLMs to generate both reasoning traces and task-specific actions in an interleaved manner. Inspired by the intricate dance between \"acting\" and \"reasoning\" that characterises human cognitive processes, ReAct Prompting represents a leap forward in enhancing the utility of large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"trip_planning_prompt = ===\n",
    "                Decision Task: Plan a Trip to Paris\n",
    "                Thought 1: I need to gather information about the destination,\n",
    "                Action 1: Search [Top attractions in Paris]\n",
    "                Thought 2: Eiffel Tower is a must-visit. I should find information about its opening hours.\n",
    "                Action 2: Lookup [Eiffel Tower opening hours)\n",
    "                Thought 3: I want to explore local cuisine. Let's find the best restaurants in Paris.\n",
    "                Action 3: Search [Best restaurants in Paris]\n",
    "             \n",
    "                Thought 4: Summarize the plan and fimalize the decisions.\n",
    "                Action 4: Finish(Finalize trip plan]\n",
    "\n",
    "           \"\"\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of thoughts prompting\n",
    "\n",
    "Chain of thoughts prompting is a technique used in natural language processing (NLP) to generate coherent and contextually relevant responses to user queries. It involves breaking down a complex task into a series of smaller, more manageable subtasks, and then generating a response to each subtask in turn.\n",
    "\n",
    "**Prompt:**\n",
    "\"Let's break this problem into steps to find the solution.\n",
    "\n",
    "1. Start with the number of apples you currently have.\n",
    "2. Add the apples you buy to this number.\n",
    "3. Subtract the apples you give away.\n",
    "Now calculate the total.\"\n",
    "\n",
    "\n",
    "**Model Response:**\n",
    "\"1. You currently have 3 apples.\n",
    "2. You buy 2 more apples, so 3 + 2 = 5.\n",
    "3. You give away 1 apple, so 5 - 1 = 4.\n",
    "Answer: You have 4 apples.\"\n",
    "\n",
    "This approach encourages the model to reason step-by-step to arrive at the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "            math_problem_prompt = ===\n",
    "            Question: If you have 10 oranges, and you eat 3, then buy 5 more, how many oranges do you have?\n",
    "            \n",
    "            Thought 1: Start with the initial number of oranges.\n",
    "            Action 1: Identify the starting amount of oranges. [10]\n",
    "            \n",
    "            Thought 2: Subtract the oranges that are eaten.\n",
    "            Action 2: Perform the subtraction. [10 - 3]\n",
    "            \n",
    "            Thought 3: Add the oranges that are bought.\n",
    "            Action 3: Perform the addition. [7 + 5]\n",
    "            \n",
    "            Thought 4: Summarize the total oranges you have.\n",
    "            Action 4: Finish (Total oranges: 12)\n",
    "            \"\"\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Hallucincation Detection\n",
    "\n",
    "Key Features of the Prompt:\n",
    "1. Explicit Instruction for Hallucination Detection: The model is guided to identify and label hallucinations explicitly.\n",
    "2. Reasoning Requirement: The model must explain why it believes a statement is true or false, promoting groundedness.\n",
    "3. Fact-Based Statements: Includes verifiable facts mixed with common misconceptions to test hallucination tendencies.\n",
    "This approach can be adapted to other domains like medical, legal, or technical fields to verify factuality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "            Evaluate the accuracy of the following statements. If any statement cannot be verified or seems fabricated, explicitly mark it as \"Potential Hallucination.\" Provide a reasoning step for your assessment.\n",
    "            \n",
    "            Statements:\n",
    "            1. The Eiffel Tower was constructed in 1889 and is located in London.\n",
    "            2. The Great Wall of China is visible from space with the naked eye.\n",
    "            3. The Amazon Rainforest is the largest rainforest in the world.\n",
    "            \n",
    "            Instructions:\n",
    "            - Identify whether each statement is factually correct or contains hallucinations.\n",
    "            - Provide reasoning for your conclusion.\n",
    "            \"\"\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing Context to Reduce Hallucinations\n",
    "\n",
    "1. Communicate clearly\n",
    "The art of prompt engineering is the art of communication. Large language models have been trained on a massive amount of written and transcribed human content. So just like communicating with people, it's critical to communicate clearly with the models. Ambiguous requests and missing details will lead to undesirable responses. Throughout these exercises, you will see examples of varying levels of detail and clarity.\n",
    "\n",
    "2. Provide context\n",
    "Depending on when and how a large language model was trained, it will often need additional information to help it respond to a prompt appropriately. Applications like chatbots and others may need to add this context to a prompt based on a user's question.\n",
    "\n",
    "The exercises below should help illustrate the importance of providing context to large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_ID,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        Human: Who is the heaviest hippo ever recorded?  \n",
    "        Assistant:\n",
    "           \"\"\"\n",
    "    }],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
