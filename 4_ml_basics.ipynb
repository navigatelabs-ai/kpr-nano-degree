{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1: Machine Learning Basics Notebook\n",
    "\n",
    "This notebook is a part of the Nano Degree in Generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Explore the Basics of Machine Learning\n",
    "\n",
    "In this notebook, we will learn about basic concepts of machine learning. We will explore following concepts to get started with machine learning. We will also learn about the different types of machine learning algorithms. Exploratory Data Analysis (EDA) is an important step in the machine learning workflow. We will also learn about these along with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Underfitting and Overfitting\n",
    "\n",
    "### Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**What is Underfitting?**\n",
    "\n",
    "Underfitting occurs when a machine learning model cannot capture the underlying patterns and relationships in the data. This can happen when the model is too simple or when the data is too complex. Underfitting can lead to poor performance on both training and test data.\n",
    "\n",
    "**How to identify underfitting?**\n",
    "\n",
    "There are several ways to identify underfitting in a machine learning model:\n",
    "1. **Training and test accuracy:** If the training accuracy is much higher than the test accuracy, it may indicate that the model is underfitting.\n",
    "2. **Visualization:** Plotting the training and test data can help identify underfitting. If the training data and test data have different distributions, it may indicate that the model is underfitting.\n",
    "3. **Cross-validation:** Using cross-validation can help identify underfitting. If the model performs poorly on all folds of the cross-validation, it may indicate that the model is underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic non-linear data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Train a linear regression model to demonstrate underfitting\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred = linear_model.predict(X)\n",
    "\n",
    "# Plotting the data and model prediction\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Linear Model Prediction')\n",
    "plt.title('Demonstrating Underfitting with Linear Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation - Mean Squared Error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Plot residuals to see the poor fit\n",
    "plt.scatter(X, y - y_pred, color='green')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot - Underfitting')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals (y - y_pred)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "**What is overfitting?**\n",
    "Overfitting is a common problem in machine learning and statistics, where a statistical model or machine learning algorithm fits the training data too closely, including noise or random fluctuations in the training data. This can lead to poor generalization to new, unseen data, as the model may be too specialized to the training data and not able to capture the underlying patterns or relationships in the data.\n",
    "\n",
    "**How to avoid overfitting?**\n",
    "There are several ways to avoid overfitting in machine learning and statistics:\n",
    "1. **Regularization**: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that is minimized during training. This penalty term encourages the model to have simpler weights, which can help prevent overfitting.\n",
    "2. **Cross-validation**: Cross-validation is a technique used to evaluate the performance of a model on unseen data. It involves splitting the data into training and validation sets, and then training the model on the training set and evaluating its performance on the validation set. This can help identify if the model is overfitting or underfitting the data.\n",
    "3. **Early stopping**: Early stopping is a technique used to prevent overfitting by stopping the training process when the model starts to overfit the training data. This can be done by monitoring the performance of the model on a validation set and stopping the training process when the performance starts to degrade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic non-linear data\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Apply polynomial features to the data to address underfitting\n",
    "polynomial_model = make_pipeline(PolynomialFeatures(degree=3), LinearRegression())\n",
    "polynomial_model.fit(X, y)\n",
    "\n",
    "# Predict using the model\n",
    "y_pred = polynomial_model.predict(X)\n",
    "\n",
    "# Plotting the data and model prediction\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Polynomial Model Prediction')\n",
    "plt.title('Mitigating Underfitting with Polynomial Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation - Mean Squared Error\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Plot residuals to see the fit\n",
    "plt.scatter(X, y - y_pred, color='green')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residual Plot - Polynomial Regression')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals (y - y_pred)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid Overfitting with Lasso Regression\n",
    "\n",
    "To avoid overfitting in the example above, we can use a polynomial regression model combined with regularization techniques like Ridge Regression or Lasso Regression. These methods add a penalty term to the model, helping to reduce the complexity and thus prevent overfitting.\n",
    "\n",
    "Here's an improved version of the code with Ridge Regression:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Create a synthetic dataset with a non-linear relationship\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)  # 100 points between 0 and 10\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])  # Sine wave with some noise\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create a polynomial regression model with regularization (Ridge Regression)\n",
    "degree = 8  # Degree of the polynomial\n",
    "alpha = 1.0  # Regularization strength (1.0 is a moderate amount of regularization)\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 5: Calculate the Mean Squared Error (MSE) to evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Step 6: Visualize the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, model.predict(X), color='red', label=f'Polynomial Model (Degree={degree}) with Ridge')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Ridge Regression to Avoid Overfitting')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
